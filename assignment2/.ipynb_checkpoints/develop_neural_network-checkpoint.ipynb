{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cHqo6b1_Bzk"
   },
   "source": [
    "# Implement a Neural Network\n",
    "\n",
    "This notebook contains testing code to help you develop a neural network by implementing the forward pass and backpropagation algorithm in the `models/neural_net.py` file. \n",
    "\n",
    "You will implement your network in the class `NeuralNetwork` inside the file `models/neural_net.py` to represent instances of the network. The network parameters are stored in the instance variable `self.params` where keys are string parameter names and values are numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nTt_CiWh_Bzm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from models.neural_net import NeuralNetwork\n",
    "\n",
    "# For auto-reloading external modules\n",
    "# See http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\"Returns relative error\"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5X9DO-5_Bzn"
   },
   "source": [
    "The cell below initializes a toy dataset and corresponding model which will allow you to check your forward and backward pass by using a numeric gradient check. Note that we set a random seed for repeatable experiments."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 25,
>>>>>>> 0ded59f (need to make sure by hand)
   "metadata": {
    "id": "358jAXcc_Bzn"
   },
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "hidden_size = 3\n",
    "num_classes = 2\n",
    "num_inputs = 3\n",
    "optimizer = 'SGD'\n",
    "\n",
    "\n",
    "def init_toy_model(num_layers):\n",
    "    \"\"\"Initializes a toy model\"\"\"\n",
    "    np.random.seed(0)\n",
    "    hidden_sizes = [hidden_size] * (num_layers - 1)\n",
    "    print(\"hidden_sizes\", hidden_sizes)\n",
    "    return NeuralNetwork(input_size, hidden_sizes, num_classes, num_layers)\n",
    "\n",
    "def init_toy_data():\n",
    "    \"\"\"Initializes a toy dataset\"\"\"\n",
    "    np.random.seed(0)\n",
    "    X = np.random.randn(num_inputs, input_size)\n",
    "    y = np.random.randn(num_inputs, num_classes)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zh_v9biP_Bzn"
   },
   "source": [
    "# Implement forward and backward pass\n",
    "\n",
    "The first thing you will do is implement the forward pass of your neural network. The forward pass should be implemented in the `forward` function. You can use helper functions like `linear`, `relu`, and `sigmoid` to help organize your code.\n",
    "\n",
    "Next, you will implement the backward pass using the backpropagation algorithm. Backpropagation will compute the gradient of the loss with respect to the model parameters `W1`, `b1`, ... etc. Use an MSE for loss calcuation. Fill in the code blocks in `NeuralNetwork.backward`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjAwpT2z_Bzo"
   },
   "source": [
    "# Gradient  check\n",
    "\n",
    "If you have implemented your forward pass through the network correctly, you can use the following cell to debug your backward pass with a numeric gradient check. If your backward pass has been implemented correctly, the max relative error between your analytic solution and the numeric solution should be around 1e-7 or less for all parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UZM47qUP_Bzo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "W1shape:  (2, 10)\n",
      "b11shape:  (10,)\n",
      "W2shape:  (10, 3)\n",
      "b22shape:  (3,)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "W1 max relative error: 1.000000e+00\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "b1 max relative error: 1.000000e+00\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "W2 max relative error: 1.000000e+00\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "b2 max relative error: 1.000000e+00\n",
      "W1shape:  (2, 10)\n",
      "b11shape:  (10,)\n",
      "W2shape:  (10, 10)\n",
      "b22shape:  (10,)\n",
      "W3shape:  (10, 3)\n",
      "b33shape:  (3,)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "W1 max relative error: 1.000000e+00\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "b1 max relative error: 1.000000e+00\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "W2 max relative error: 1.000000e+00\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "b2 max relative error: 1.000000e+00\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "W3 max relative error: 1.000000e+00\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "b3 max relative error: 1.000000e+00\n"
=======
      "hidden_sizes [3]\n",
      "(3, 2)\n",
      "mse grad [[-0.29943483  0.43676576]\n",
      " [ 0.40286197  0.06216569]\n",
      " [ 0.2378799  -0.6336675 ]]\n",
      "Sigmoid grad [[0.24447886 0.23844631]\n",
      " [0.24012459 0.24975862]\n",
      " [0.24649641 0.2264928 ]]\n",
      "(3, 2)\n",
      "mse grad [[-0.29943483  0.43676576]\n",
      " [ 0.40286197  0.06216569]\n",
      " [ 0.2378799  -0.6336675 ]]\n",
      "Sigmoid grad [[0.24447886 0.23844631]\n",
      " [0.24012459 0.24975862]\n",
      " [0.24649641 0.2264928 ]]\n",
      "(3, 2)\n",
      "mse grad [[-0.29943482  0.43676576]\n",
      " [ 0.40286197  0.06216569]\n",
      " [ 0.23787992 -0.63366749]]\n",
      "Sigmoid grad [[0.24447886 0.23844631]\n",
      " [0.24012459 0.24975862]\n",
      " [0.24649641 0.2264928 ]]\n",
      "(3, 2)\n",
      "mse grad [[-0.29943485  0.43676575]\n",
      " [ 0.40286196  0.06216569]\n",
      " [ 0.23787989 -0.6336675 ]]\n",
      "Sigmoid grad [[0.24447886 0.23844631]\n",
      " [0.24012459 0.24975862]\n",
      " [0.24649641 0.2264928 ]]\n",
      "(3, 2)\n",
      "mse grad [[-0.29943479  0.43676578]\n",
      " [ 0.40286199  0.0621657 ]\n",
      " [ 0.2378799  -0.6336675 ]]\n",
      "Sigmoid grad [[0.24447886 0.23844631]\n",
      " [0.24012459 0.24975862]\n",
      " [0.24649641 0.2264928 ]]\n",
      "(3, 2)\n",
      "mse grad [[-0.29943488  0.43676573]\n",
      " [ 0.40286194  0.06216568]\n",
      " [ 0.2378799  -0.6336675 ]]\n",
      "Sigmoid grad [[0.24447886 0.23844631]\n",
      " [0.24012459 0.24975862]\n",
      " [0.24649641 0.2264928 ]]\n",
      "(3, 2)\n",
      "mse grad [[-0.29943483  0.43676577]\n",
      " [ 0.40286197  0.06216569]\n",
      " [ 0.23787991 -0.63366748]]\n",
      "Sigmoid grad [[0.24447886 0.23844631]\n",
      " [0.24012459 0.24975862]\n",
      " [0.24649641 0.2264928 ]]\n",
      "(3, 2)\n",
      "mse grad [[-0.29943484  0.43676574]\n",
      " [ 0.40286197  0.06216569]\n",
      " [ 0.2378799  -0.63366751]]\n",
      "Sigmoid grad [[0.24447886 0.23844631]\n",
      " [0.24012459 0.24975862]\n",
      " [0.24649641 0.2264928 ]]\n",
      "(3, 2)\n",
      "mse grad [[-0.29943483  0.43676576]\n",
      " [ 0.40286198  0.0621657 ]\n",
      " [ 0.2378799  -0.6336675 ]]\n",
      "Sigmoid grad [[0.24447886 0.23844631]\n",
      " [0.24012459 0.24975862]\n",
      " [0.24649641 0.2264928 ]]\n",
      "(3, 2)\n",
      "mse grad [[-0.29943484  0.43676575]\n",
      " [ 0.40286195  0.06216569]\n",
      " [ 0.23787991 -0.6336675 ]]\n",
      "Sigmoid grad [[0.24447886 0.23844631]\n",
      " [0.24012459 0.24975862]\n",
      " [0.24649641 0.2264928 ]]\n",
      "(3, 2)\n",
      "mse grad [[-0.29943483  0.43676576]\n",
      " [ 0.40286202  0.06216572]\n",
      " [ 0.2378799  -0.6336675 ]]\n",
      "Sigmoid grad [[0.24447886 0.23844631]\n",
      " [0.24012459 0.24975862]\n",
      " [0.24649641 0.2264928 ]]\n",
      "(3, 2)\n",
      "mse grad [[-0.29943484  0.43676575]\n",
      " [ 0.40286191  0.06216566]\n",
      " [ 0.2378799  -0.6336675 ]]\n",
      "Sigmoid grad [[0.24447886 0.23844631]\n",
      " [0.2401246  0.24975862]\n",
      " [0.24649641 0.2264928 ]]\n",
      "(3, 2)\n",
      "mse grad [[-0.29943483  0.43676576]\n",
      " [ 0.40286197  0.06216569]\n",
      " [ 0.2378799  -0.6336675 ]]\n",
      "Sigmoid grad [[0.24447886 0.23844631]\n",
      " [0.24012459 0.24975862]\n",
      " [0.24649641 0.2264928 ]]\n",
      "(3, 2)\n",
      "mse grad [[-0.29943484  0.43676575]\n",
      " [ 0.40286197  0.06216569]\n",
      " [ 0.23787991 -0.63366749]]\n",
      "Sigmoid grad [[0.24447886 0.23844631]\n",
      " [0.24012459 0.24975862]\n",
      " [0.24649641 0.2264928 ]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,3) (2,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_name \u001b[38;5;129;01min\u001b[39;00m net\u001b[38;5;241m.\u001b[39mparams:\n\u001b[0;32m     24\u001b[0m     param_grad_num \u001b[38;5;241m=\u001b[39m eval_numerical_gradient(f, net\u001b[38;5;241m.\u001b[39mparams[param_name], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m max relative error: \u001b[39m\u001b[38;5;132;01m%e\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (param_name, \u001b[43mrel_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_grad_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradients_after\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m))\n",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m, in \u001b[0;36mrel_error\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrel_error\u001b[39m(x, y):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns relative error\"\"\"\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39mabs(\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m) \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m1e-8\u001b[39m, np\u001b[38;5;241m.\u001b[39mabs(x) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(y))))\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,3) (2,10) "
>>>>>>> 0ded59f (need to make sure by hand)
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from utils.gradient_check import eval_numerical_gradient\n",
    "\n",
    "X, y = init_toy_data()\n",
    "\n",
    "\n",
    "def f(W):\n",
    "    net.forward(X)\n",
    "    return net.backward(y)\n",
    "\n",
    "# for num in [2, 3]:\n",
    "for num in [2]:\n",
    "    net = init_toy_model(num)\n",
    "    params = deepcopy(net.params)\n",
    "\n",
    "    prediction = net.forward(X)\n",
    "    outputs = deepcopy(net.outputs)\n",
    "    \n",
    "    loss = net.backward(y)\n",
    "    gradients = deepcopy(net.gradients)\n",
    "\n",
    "    for param_name in net.params:\n",
    "        param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "        print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, gradients_after[param_name])))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
=======
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.01764052,  0.00400157,  0.00978738],\n",
       "        [ 0.02240893,  0.01867558, -0.00977278]]),\n",
       " 'b1': array([ 0.00950088, -0.00151357, -0.00103219]),\n",
       " 'W2': array([[0.00410599, 0.00144044],\n",
       "        [0.01454274, 0.00761038],\n",
       "        [0.00121675, 0.00443863]]),\n",
       " 'b2': array([0.00333674, 0.01494079])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Result0': array([[ 1.76405235,  0.40015721],\n",
       "        [ 0.97873798,  2.2408932 ],\n",
       "        [ 1.86755799, -0.97727788]]),\n",
       " 'Linear1': array([[ 0.04958679,  0.01301858,  0.01232261],\n",
       "        [ 0.07698236,  0.0442529 , -0.01335266],\n",
       "        [ 0.02054583, -0.01229164,  0.02679703]]),\n",
       " 'Result1': array([[0.04958679, 0.01301858, 0.01232261],\n",
       "        [0.07698236, 0.0442529 , 0.        ],\n",
       "        [0.02054583, 0.        , 0.02679703]]),\n",
       " 'Linear2': array([[0.00374467, 0.01516599],\n",
       "        [0.00429639, 0.01538846],\n",
       "        [0.00345371, 0.01508933]]),\n",
       " 'Sigmoid2': array([[0.50093617, 0.50379142],\n",
       "        [0.5010741 , 0.50384704],\n",
       "        [0.50086343, 0.50377226]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sigmoid2': array([[0.24447886, 0.23844631],\n",
       "        [0.24012459, 0.24975862],\n",
       "        [0.24649641, 0.2264928 ]]),\n",
       " 'W2': array([[0.03567275, 0.03570428],\n",
       "        [0.01380898, 0.01415677],\n",
       "        [0.00961799, 0.00900762]]),\n",
       " 'b2': array([0.73443661, 0.72963851]),\n",
       " 'W1': array([[ 0.00619326,  0.01475123,  0.0048294 ],\n",
       "        [ 0.00224677,  0.01423361, -0.00073303]]),\n",
       " 'b1': array([0.01353225, 0.00924931, 0.0016289 ])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.76405235,  0.40015721],\n",
       "       [ 0.97873798,  2.2408932 ],\n",
       "       [ 1.86755799, -0.97727788]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.95008842, -0.15135721],\n",
       "       [-0.10321885,  0.4105985 ],\n",
       "       [ 0.14404357,  1.45427351]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
>>>>>>> 0ded59f (need to make sure by hand)
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.9.16"
=======
   "version": "3.10.9"
>>>>>>> 0ded59f (need to make sure by hand)
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
